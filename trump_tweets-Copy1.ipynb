{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53a15392d366de67d77bb0748b7775c0",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Trump, Twitter, and the 2016 Election\n",
    "\n",
    "In this project, I work with the Twitter API and look at Trump's tweets, along with a data set of tweets from Russian bot accounts. Note this does *not* include those uncovered and deleted recently by twitter due to the current investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a1c52c1b5855b6bd9b10e6a7802072",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69812bdeba1101e72f0c2c49d4e05e7e",
     "grade": false,
     "grade_id": "tweepy-intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Downloading Recent Tweets\n",
    "\n",
    "---\n",
    "\n",
    "Since we'll be looking at Twitter data, we need to download the data from Twitter!\n",
    "\n",
    "Twitter provides an API for downloading tweet data in large batches.  The `tweepy` package makes it fairly easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "727d86148491e8268741fb9ad1182f32",
     "grade": false,
     "grade_id": "tweepy",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Make sure you are in your data100 conda environment if you are working locally.\n",
    "# The following should run:\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be71d5e18529a33a68df98dba32c6b98",
     "grade": false,
     "grade_id": "instructions",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "There are instructions on using `tweepy` [here](http://tweepy.readthedocs.io/en/v3.5.0/getting_started.html), but we will give you example code.\n",
    "\n",
    "Twitter requires you to have authentication keys to access their API.  To get your keys, you'll have to sign up as a Twitter developer.  The next question will walk you through this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e6251507833774020ae3a92b3ece63f",
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Follow the instructions below to get your Twitter API keys.  **Read the instructions completely before starting.**\n",
    "\n",
    "1. [Create a Twitter account](https://twitter.com).  You can use an existing account if you have one; if you prefer to not do this assignment under your regular account, feel free to create a throw-away account.\n",
    "2. Under account settings, add your phone number to the account.\n",
    "3. [Create a Twitter developer account](https://dev.twitter.com/resources/signup).  Attach it to your Twitter account.\n",
    "4. Once you're logged into your developer account, [create an application for this assignment](https://apps.twitter.com/app/new).  You can call it whatever you want, and you can write any URL when it asks for a web site.  You don't need to provide a callback URL.\n",
    "5. On the page for that application, find your Consumer Key and Consumer Secret.\n",
    "6. On the same page, create an Access Token.  Record the resulting Access Token and Access Token Secret.\n",
    "7. Edit the file [keys.json](keys.json) and replace the placeholders with your keys.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1bb1fee574ae618469603216cbed678",
     "grade": false,
     "grade_id": "warning",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## WARNING (Please Read) !!!!\n",
    "\n",
    "\n",
    "### Protect your Twitter Keys\n",
    "<span style=\"color:red\">\n",
    "If someone has your authentication keys, they can access your Twitter account and post as you!  So don't give them to anyone, and **don't write them down in this notebook**. \n",
    "</span>\n",
    "The usual way to store sensitive information like this is to put it in a separate file and read it programmatically.  That way, you can share the rest of your code without sharing your keys.  That's why we're asking you to put your keys in `keys.json` for this assignment.\n",
    "\n",
    "\n",
    "### Avoid making too many API calls.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Twitter limits developers to a certain rate of requests for data.  If you make too many requests in a short period of time, you'll have to wait awhile (around 15 minutes) before you can make more.  </span> \n",
    "So carefully follow the code examples you see and don't rerun cells without thinking.  Instead, always save the data you've collected to a file.  We've provided templates to help you do that.\n",
    "\n",
    "\n",
    "### Be careful about which functions you call!\n",
    "\n",
    "<span style=\"color:red\">\n",
    "This API can retweet tweets, follow and unfollow people, and modify your twitter settings.  Be careful which functions you invoke! </span> One of your instructors accidentally re-tweeted some tweets because that instructor typed `retweet` instead of `retweet_count`. \n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2eb5fb9a1af6832165a365b0aad21ab",
     "grade": false,
     "grade_id": "keys",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "key_file = 'keys.json'\n",
    "# Loading your keys from keys.json (which you should have filled\n",
    "# in in question 1):\n",
    "with open(key_file) as f:\n",
    "    keys = json.load(f)\n",
    "# if you print or view the contents of keys be sure to delete the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "118d21c4ccbbf942c9f2707840908cca",
     "grade": false,
     "grade_id": "tweepy-auth-note",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "This cell tests the Twitter authentication. It should run without errors or warnings and display your Twitter username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66806331fe1cb852ab300e98459cb2d4",
     "grade": false,
     "grade_id": "twitter-auth",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import TweepError\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "    print(\"Your username is:\", api.auth.get_username())\n",
    "except TweepError as e:\n",
    "    logging.warning(\"There was a Tweepy error. Double check your API keys and try again.\")\n",
    "    logging.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b738ef5c6ffad625b8d5a61b66719ad6",
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "In the example below, we have loaded some tweets by @BerkeleyData.  Run it and read the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3defce91d655ee59ae318cc20133476a",
     "grade": false,
     "grade_id": "load-berkeley-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ds_tweets_save_path = \"BerkeleyData_recent_tweets.json\"\n",
    "# Guarding against attempts to download the data multiple\n",
    "# times:\n",
    "if not Path(ds_tweets_save_path).is_file():\n",
    "    # Getting as many recent tweets by @BerkeleyData as Twitter will let us have.\n",
    "    # We use tweet_mode='extended' so that Twitter gives us full 280 character tweets.\n",
    "    # This was a change introduced in September 2017.\n",
    "    \n",
    "    # The tweepy Cursor API actually returns \"sophisticated\" Status objects but we \n",
    "    # will use the basic Python dictionaries stored in the _json field. \n",
    "    example_tweets = [t._json for t in tweepy.Cursor(api.user_timeline, id=\"BerkeleyData\", \n",
    "                                             tweet_mode='extended').items()]\n",
    "    \n",
    "    # Saving the tweets to a json file on disk for future analysis\n",
    "    with open(ds_tweets_save_path, \"w\") as f:        \n",
    "        json.dump(example_tweets, f)\n",
    "\n",
    "# Re-loading the json file:\n",
    "with open(ds_tweets_save_path, \"r\") as f:\n",
    "    example_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dc93f6841b3ecdb826a8ff12ebb2030",
     "grade": false,
     "grade_id": "first-tweet-did-it-workk",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Assuming everything ran correctly you should be able to look at the first tweet by running the cell below.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "**Warning** Do not attempt to view all the tweets in a notebook.  It will likely freeze your browser.  The following would be a **bad idea**:\n",
    "```python\n",
    "    pprint(example_tweets)\n",
    "```\n",
    "\n",
    "</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de383bb55c2e4e13c896209b5f7589c0",
     "grade": false,
     "grade_id": "first-tweet",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Looking at one tweet object, which has type Status: \n",
    "from pprint import pprint # ...to get a more easily-readable view.\n",
    "pprint(example_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43ce65ddda5bb1543856611dd2c536f9",
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 2a\n",
    "\n",
    "### What you need to do. \n",
    "\n",
    "Re-factor the above code fragment into reusable snippets below.  You should not need to make major modifications; this is mostly an exercise in understanding the above code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd6de9d61c340450539455afd0f8fdf3",
     "grade": false,
     "grade_id": "load-keys",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "    \"\"\"Loads your Twitter authentication keys from a file on disk.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to your key file.  The file should\n",
    "          be in JSON format and look like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping key names (like \"consumer_key\") to\n",
    "          key values.\"\"\"\n",
    "    key_file = path\n",
    "    with open(key_file) as file:\n",
    "        auth_keys = json.load(file)\n",
    "    return auth_keys\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76ff200bf064ab3f5c457bc4725d8cd8",
     "grade": false,
     "grade_id": "download-recent-tweets",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def download_recent_tweets_by_user(user_account_name, key_file):\n",
    "    \"\"\"Downloads tweets by one Twitter user.\n",
    "\n",
    "    Args:\n",
    "        user_account_name (str): The name of the Twitter account\n",
    "          whose tweets will be downloaded.\n",
    "        keys (dict): A Python dictionary with Twitter authentication\n",
    "          keys (strings), like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictonary objects, each representing one tweet.\"\"\"\n",
    "    \n",
    "    tweets = [t._json for t in tweepy.Cursor(api.user_timeline, id=user_account_name, tweet_mode='extended').items()]\n",
    "    return tweets\n",
    "    \n",
    "    \"\"\" IGNORE BELOW SAVING IN CASE NEED TO EDIT!\n",
    "    save_path = user_account_name + \"_latest_tweets.json\"\n",
    "    # Guard against many attempts to download data and overload requests\n",
    "    if not Path(save_path).is_file():\n",
    "        tweets = [t._json for t in tweepy.Cursor(api.user_timeline, id=user_account_name, tweet_mode='extended').items()]\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(tweets, f)\n",
    "    with open(save_path) as f:\n",
    "        tweets = json.load(f)\n",
    "    return tweets\"\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75d788899e9212031d95685b7ebb4505",
     "grade": false,
     "grade_id": "save-tweets",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def save_tweets(tweets, path):\n",
    "    \"\"\"Saves a list of tweets to a file in the local filesystem.\n",
    "    \n",
    "    This function makes no guarantee about the format of the saved\n",
    "    tweets, **except** that calling load_tweets(path) after\n",
    "    save_tweets(tweets, path) will produce the same list of tweets\n",
    "    and that only the file at the given path is used to store the\n",
    "    tweets.  (That means you can implement this function however\n",
    "    you want, as long as saving and loading works!)\n",
    "\n",
    "    Args:\n",
    "        tweets (list): A list of tweet objects (of type Dictionary) to\n",
    "          be saved.\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(tweets, f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31f5a74addf3b0211c65b0220160568c",
     "grade": false,
     "grade_id": "load-tweets",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets were be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictionary objects, each representing one tweet.\"\"\"\n",
    "    with open(path) as f:\n",
    "        tweets = json.load(f)\n",
    "    return tweets\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c2633d5128de6e4aadf225aa291118d",
     "grade": false,
     "grade_id": "get-tweets-with-cache",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def get_tweets_with_cache(user_account_name, keys_path):\n",
    "    \"\"\"Get recent tweets from one user, loading from a disk cache if available.\n",
    "    \n",
    "    The first time you call this function, it will download tweets by\n",
    "    a user.  Subsequent calls will not re-download the tweets; instead\n",
    "    they'll load the tweets from a save file in your local filesystem.\n",
    "    All this is done using the functions you defined in the previous cell.\n",
    "    This has benefits and drawbacks that often appear when you cache data:\n",
    "    \n",
    "    +: Using this function will prevent extraneous usage of the Twitter API.\n",
    "    +: You will get your data much faster after the first time it's called.\n",
    "    -: If you really want to re-download the tweets (say, to get newer ones,\n",
    "       or because you screwed up something in the previous cell and your\n",
    "       tweets aren't what you wanted), you'll have to find the save file\n",
    "       (which will look like <something>_recent_tweets.pkl) and delete it.\n",
    "    \n",
    "    Args:\n",
    "        user_account_name (str): The Twitter handle of a user, without the @.\n",
    "        keys_path (str): The path to a JSON keys file in your filesystem.\n",
    "    \"\"\"\n",
    "    save_path = user_account_name + \"_recent_tweets.json\"\n",
    "    if not Path(save_path).is_file():\n",
    "        tweets = download_recent_tweets_by_user(user_account_name, keys_path)\n",
    "        save_tweets(tweets, save_path)\n",
    "    return load_tweets(save_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2ddaff40134b8cf8f582c1df5db3883",
     "grade": false,
     "grade_id": "about-to-get-real",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "If everything was implemented correctly you should be able to obtain roughly the last 3000 tweets by the `realdonaldtrump`.  (This may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac3bb2120526571535abbe412929bfd",
     "grade": false,
     "grade_id": "trump-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# When you are done, run this cell to load @realdonaldtrump's tweets.\n",
    "# Note the function get_tweets_with_cache.  You may find it useful\n",
    "# later.\n",
    "trump_tweets = get_tweets_with_cache(\"realdonaldtrump\", key_file)\n",
    "print(\"Number of tweets downloaded:\", len(trump_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afaf66b3188a4752b0271beed957ef6c",
     "grade": true,
     "grade_id": "trump-tweets-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 2000 <= len(trump_tweets) <= 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e991a5d45fd0eeebe8b1c6371a4019f9",
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2b\n",
    "\n",
    "We are limited to how many tweets we can download.  In what month is the oldest tweet from Trump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oldest tweet:\n",
    "trump_tweets[len(trump_tweets)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b46b55114772326e527b1dd67a17a8e",
     "grade": false,
     "grade_id": "oldest-month-question",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Enter the number of the month of the oldest tweet (e.g. 1 for January)\n",
    "oldest_month = 10 \n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1bc7ba95e8b2c50def1081519525604",
     "grade": true,
     "grade_id": "oldest-month-answer",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06c75046cf9899a309ab35c11c403965",
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## Question 3\n",
    "\n",
    "\n",
    "**IMPORTANT! PLEASE READ**\n",
    "\n",
    "Unfortunately, Twitter prevent us from going further back in time using the public APIs.  Fortunately, we have a snapshot of earlier tweets that we can combine with our new data.  \n",
    "\n",
    "We will again use the `fetch_and_cache` utility to download the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a56fd6ade293905b7cfe91c16c54c650",
     "grade": false,
     "grade_id": "download-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "from utils import fetch_and_cache\n",
    "data_url = 'http://www.ds100.org/sp18/assets/datasets/old_trump_tweets.json.zip'\n",
    "file_name = 'old_trump_tweets.json.zip'\n",
    "\n",
    "dest_path = fetch_and_cache(data_url=data_url, file=file_name)\n",
    "print(f'Located at {dest_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da803030a1c84a39afbb134dfb8b19fd",
     "grade": false,
     "grade_id": "loading-data-inst",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, we we will load the tweets directly from the compressed file without decompressing it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f84519e987875b6b57d9cc5bae460cf",
     "grade": false,
     "grade_id": "loading-old-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "my_zip = zipfile.ZipFile(dest_path, 'r')\n",
    "with my_zip.open(\"old_trump_tweets.json\", \"r\") as f:\n",
    "    old_trump_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8661fc27e5f1cdfce041efd3896ae838",
     "grade": false,
     "grade_id": "formatting-note",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "This data is formatted identically to the recent tweets we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d8ef8fa2c1b900a48779e6cf0320b6",
     "grade": false,
     "grade_id": "pprint-old-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pprint(old_trump_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15f652a6c9d0626c82b34be4253b2333",
     "grade": false,
     "grade_id": "listing-keys-inst",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "As a dictionary we can also list the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88eaf1ba38e6c52151ad6a2922216c0",
     "grade": false,
     "grade_id": "listing-keys",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "old_trump_tweets[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b40bfbf5e74ba5af630326ab7a77c98c",
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3a\n",
    "\n",
    "Merge the `old_trump_tweets` and the `trump_tweets` we downloaded from twitter into one giant list of tweets. \n",
    "\n",
    "**Important:** There may be some overlap so be sure to eliminate duplicate tweets.  \n",
    "**Hint:** the `id` of a tweet is always unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfff2cf8e110d51cb3dbb7f5086b9be4",
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create list of ids in trump_tweets\n",
    "ids = []\n",
    "for x in trump_tweets:\n",
    "    if x.get('id') not in ids:\n",
    "        ids.append(x.get('id'))\n",
    "\n",
    "# set tweet_lst to all trump tweets (because these IDs are unique)\n",
    "# make sure to not modify trump_tweets below!\n",
    "tweet_lst = trump_tweets[:]\n",
    "\n",
    "# Now add all old_trump_tweets who's ids are NOT in id tracker.\n",
    "for y in old_trump_tweets:\n",
    "    if y.get('id') not in ids:\n",
    "        ids.append(y.get('ds'))\n",
    "        tweet_lst.append(y)\n",
    "        \n",
    "# Here we are:\n",
    "all_tweets = tweet_lst\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a21da10aa86da615b340c8edb2973c3",
     "grade": true,
     "grade_id": "q3a-test",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(all_tweets) > len(trump_tweets)\n",
    "assert len(all_tweets) > len(old_trump_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "532cecf04260fcadca43ce63bf296d5d",
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3b\n",
    "\n",
    "Construct a DataFrame called `trump` containing all the tweets stored in `all_tweets`. The index of the dataframe should be the ID of each tweet (looks something like `907698529606541312`). It should have these columns:\n",
    "\n",
    "- `time`: The time the tweet was created encoded as a datetime object. (Use `pd.to_datetime` to encode the timestamp.)\n",
    "- `source`: The source device of the tweet.\n",
    "- `text`: The text of the tweet.\n",
    "- `retweet_count`: The retweet count of the tweet. \n",
    "\n",
    "Finally, **the resulting dataframe should be sorted by the index.**\n",
    "\n",
    "**Warning:** *Some tweets will store the text in the `text` field and other will use the `full_text` field.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF with all keys\n",
    "all_info = pd.DataFrame(all_tweets).set_index('id')\n",
    "\n",
    "# Narrow it down to fields we care about; NOTE DOES NOT INCLUDE FULL_TEXT YET!\n",
    "filtered = all_info[['created_at', 'source', 'text', 'retweet_count']]\n",
    "\n",
    "# Reformate dates\n",
    "reformated_dates = pd.to_datetime(filtered['created_at'])\n",
    "filtered['created_at'] = reformated_dates\n",
    "filtered.rename(columns={\"created_at\": \"time\"}, inplace=True)\n",
    "\n",
    "# merge text and full_text\n",
    "filtered['text'].fillna(all_info['full_text'], inplace=True)\n",
    "filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbe93f4346b0c4ce93681c2c7d06078",
     "grade": false,
     "grade_id": "q3b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Create DF with all keys\n",
    "all_info = pd.DataFrame(all_tweets).set_index('id')\n",
    "\n",
    "# Narrow it down to fields we care about; NOTE DOES NOT INCLUDE FULL_TEXT YET!\n",
    "filtered = all_info[['created_at', 'source', 'text', 'retweet_count']]\n",
    "\n",
    "# Reformate dates\n",
    "reformated_dates = pd.to_datetime(filtered['created_at'])\n",
    "filtered['created_at'] = reformated_dates\n",
    "filtered.rename(columns={\"created_at\": \"time\"}, inplace=True)\n",
    "\n",
    "# merge text and full_text (recall twitter added more characters so text is NaN afterwards)\n",
    "filtered['text'].fillna(all_info['full_text'], inplace=True)\n",
    "filtered\n",
    "\n",
    "# Done\n",
    "trump = filtered\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d870180ff630d9ce182d04cd033fe1b2",
     "grade": true,
     "grade_id": "q3b-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q03"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(trump, pd.DataFrame)\n",
    "assert trump.shape[0] < 8000\n",
    "assert trump.shape[1] >= 4\n",
    "assert 831846101179314177 in trump.index\n",
    "assert 753063644578144260 in trump.index\n",
    "assert all(col in trump.columns for col in ['time', 'source', 'text', 'retweet_count'])\n",
    "# If you fail these tests, you probably tried to use __dict__ or _json to read in the tweets\n",
    "assert np.sometrue([('Twitter for iPhone' in s) for s in trump['source'].unique()])\n",
    "assert trump['time'].dtype == np.dtype('<M8[ns]')\n",
    "assert trump['text'].dtype == np.dtype('O')\n",
    "assert trump['retweet_count'].dtype == np.dtype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30d38d0c0f5988b2b1da8eadd678d083",
     "grade": false,
     "grade_id": "question4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4: Tweet Source Analysis\n",
    "\n",
    "In the following questions, we are going to find out the charateristics of Trump tweets and the devices used for the tweets.\n",
    "\n",
    "First let's examine the source field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6afb6dfe20e5154416906478ba3d9384",
     "grade": false,
     "grade_id": "unique-sources",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trump['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6456dce1f6278ee550862faf8829f32",
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4a\n",
    "\n",
    "Remove the HTML tags from the source field. \n",
    "\n",
    "**Hint:** Use `trump['source'].str.replace` and your favorite regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of first <, then get rid of >\n",
    "reformated_source = trump['source'].str.replace('<[^>]+>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a05e9dfd89d13b6b4d93b7b32b0c18d0",
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Get rid of first <, then get rid of >\n",
    "reformated_source = trump['source'].str.replace('<[^>]+>', '')\n",
    "\n",
    "trump['source'] = reformated_source\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba180bc3697692747a71297f6b4dc64a",
     "grade": true,
     "grade_id": "q4a-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ELEC_DATE = datetime(2016, 11, 8)\n",
    "INAUG_DATE = datetime(2017, 1, 20)\n",
    "assert set(trump[(trump['time'] > ELEC_DATE) & (trump['time'] < INAUG_DATE) ]['source'].unique()) == set(['Twitter Ads',\n",
    " 'Twitter Web Client',\n",
    " 'Twitter for Android',\n",
    " 'Twitter for iPhone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbd18f3db9b76174657bbfadec0dede6",
     "grade": false,
     "grade_id": "note-about-device-usage",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We can see in the following plot that there are two device types that are more commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80767c65e51906a26ac3aa9c6e0d48e5",
     "grade": false,
     "grade_id": "device-usage-plot",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trump['source'].value_counts().plot(kind=\"bar\")\n",
    "plt.ylabel(\"Number of Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fc8de3b5e3282ab8fa3c8f31fa6794c",
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4b\n",
    "\n",
    "\n",
    "Is there a difference between his Tweet behavior across these devices? We will attempt to answer this question in our subsequent analysis.\n",
    "\n",
    "First, we'll take a look at whether Trump's tweets from an Android come at different times than his tweets from an iPhone. Note that Twitter gives us his tweets in the [UTC timezone](https://www.wikiwand.com/en/List_of_UTC_time_offsets) (notice the `+0000` in the first few tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a6257dbd02d1af40cdfd288d7c37250",
     "grade": false,
     "grade_id": "tweet-created-at",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "for t in trump_tweets[0:3]:\n",
    "    print(t['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a4f73d1c7135ddc404472884d7ba6fa",
     "grade": false,
     "grade_id": "convert-to-est-justification",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We'll convert the tweet times to US Eastern Time, the timezone of New York and Washington D.C., since those are the places we would expect the most tweet activity from Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6adc54fdcb7560cb4586d97800b8bfab",
     "grade": false,
     "grade_id": "convert-to-est",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trump['est_time'] = (\n",
    "    trump['time'].dt.tz_localize(\"UTC\") # Set initial timezone to UTC\n",
    "                 .dt.tz_convert(\"EST\") # Convert to Eastern Time\n",
    ")\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62a7fe09ae5f6e81c5112a2052452467",
     "grade": false,
     "grade_id": "need-to-do",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "**What you need to do:**\n",
    "\n",
    "Add a column called `hour` to the `trump` table which contains the hour of the day as floating point number computed by:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06c6a53e7388e12148e57710efe30726",
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# create array of each hour entry\n",
    "hour = []\n",
    "for x in trump['est_time']:\n",
    "    x = x.hour + (x.minute / 60) + x.second / (60 * 60)\n",
    "    hour.append(x)\n",
    "    \n",
    "trump['hour'] = hour\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0fcc19ae4e12541fa73e3e62f334e01",
     "grade": true,
     "grade_id": "q4b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q04a"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.isclose(trump.loc[690171032150237184]['hour'], 8.93639)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b98f2333e8e06595af89608f654a347",
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 4c\n",
    "\n",
    "Use this data along with the seaborn `distplot` function to examine the distribution over hours of the day in `easter time` that trump tweets on each device for the 2 most commonly used devices.  Your plot should look similar to the following. \n",
    "\n",
    "<img src=\"images/device_hour2.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_tweets = trump[trump['source'] == 'Twitter for iPhone']\n",
    "android_tweets = trump[trump['source'] == 'Twitter for Android']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8098891f5ad326482ed42b89fb116b10",
     "grade": true,
     "grade_id": "q4c-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### make your plot here\n",
    "phoneplt = sns.distplot(iphone_tweets['hour'], hist=False, label='Twitter for iPhone')\n",
    "phoneplt = sns.distplot(android_tweets['hour'], hist=False, label='Twitter for Android')\n",
    "phoneplt.set(ylabel='fraction')\n",
    "plt.title('Time of use by device')\n",
    "\n",
    "\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15705789bde1b146f0119a393984387c",
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "\n",
    "## Question 4d\n",
    "\n",
    "Are there any striking differences between these curves.  If someone told you that Trump tends to tweet early in the morning and then later in the evening, which device might you conclude is most likely his?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd2a71cbda4d42eaf1a86490a723f4ea",
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "source": [
    "There certainly are. Trump's infamous early morning tweets seem to come more from his Android device. This may mean he uses it as a personal device (as it is in his personal bedroom away from aides, etc.). During work, it seems he uses an iPhone. Perhaps the US government favors this for security. When it gets late in the day, Trump tweets less--going to bed (or maybe his audience is asleep so he does not use twitter!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a59afe12404b1f0811e16801674c53cf",
     "grade": false,
     "grade_id": "q5",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "Let's now look at which device he has used over the entire time period of this dataset.\n",
    "\n",
    "To examine the distribution of dates we will convert the date to a fractional year that can be plotted as a distribution.\n",
    "\n",
    "(Code borrowed from https://stackoverflow.com/questions/6451655/python-how-to-convert-datetime-dates-to-decimal-years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "473e641d026c095c438d79d7aa94b2f2",
     "grade": false,
     "grade_id": "fractional-year",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "\n",
    "trump['year'] = trump['time'].apply(year_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64eb067a351f92005ef87b72f2298f8a",
     "grade": false,
     "grade_id": "q5a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5a\n",
    "\n",
    "Use the `sns.distplot` to overlay the distributions of the 2 most frequently used web technologies over the years.  Your final plot should look like:\n",
    "\n",
    "<img src=\"images/source_years.png\" width=\"600px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e4c5f933e6074bd85bb221f71bf9e9b",
     "grade": true,
     "grade_id": "q5a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Update device tweets using data above\n",
    "iphone_tweets = trump[trump['source'] == 'Twitter for iPhone']\n",
    "android_tweets = trump[trump['source'] == 'Twitter for Android']\n",
    "\n",
    "sns.distplot(iphone_tweets['year'], label = 'Twitter for iPhone')\n",
    "sns.distplot(android_tweets['year'], label = 'Twitter for Android')\n",
    "plt.legend()\n",
    "plt.xlabel('fraction')\n",
    "plt.title('Distribution of most used devices over years')\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59c4beefb2256bd16dbd81c30cd07046",
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5b\n",
    "According to the plot, Trump's tweets come from many different sources. It turns out that many of his tweets were not from Trump himself but from his staff. [Take a look at this Verge article.](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android)\n",
    "\n",
    "Does the data support the information in the article? What else do you find out about changes in Trump's tweets sources from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "082503c0b5857367ca42491826837448",
     "grade": true,
     "grade_id": "q5b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "source": [
    "It certainly does support the rumor that Trump is now using a more secure iPhone. We see that right at the start of 2017 (his inauguration), all tweets started coming from an iPhone while none from his Android device. This is exaclty what was described in the article. Also, most of his tweets from Android are likely from him, as they are more entertaining! His staff likely used the iPhone prior to January 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9160f63c38f2638e31ad51b3260bef67",
     "grade": false,
     "grade_id": "q6-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6: Sentiment Analysis\n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bdf61c45c5bf6f12052419af685d59e",
     "grade": false,
     "grade_id": "head-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "631d386141906603b7febb8ce0901638",
     "grade": false,
     "grade_id": "q6a-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6a\n",
    "\n",
    "As you can see, the lexicon contains emojis too! The first column of the lexicon is the *token*, or the word itself. The second column is the *polarity* of the word, or how positive / negative it is.\n",
    "\n",
    "(How did they decide the polarities of these words? What are the other two columns in the lexicon? See the link above.)\n",
    "\n",
    " Read in the lexicon into a DataFrame called `sent`. The index of the DF should be the tokens in the lexicon. `sent` should have one column: `polarity`: The polarity of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.read_table('vader_lexicon.txt', names = ['token', 'polarity', 'sd', 'sentiment' ])\n",
    "vader.drop(['sd', 'sentiment'], axis = 1, inplace=True)\n",
    "vader.set_index('token', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "962759aad285ccfb16f06dc8dd86035c",
     "grade": false,
     "grade_id": "q6a1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "sent = vader\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "417f93b4eb89b4206d6b77725b72e56a",
     "grade": true,
     "grade_id": "q6a-test1",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q05a"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(sent, pd.DataFrame)\n",
    "assert sent.shape == (7517, 1)\n",
    "assert list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons']\n",
    "assert np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53a09bb6ee188a572ee0c8501087ac9c",
     "grade": false,
     "grade_id": "q6b-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6b\n",
    "\n",
    "Now, let's use this lexicon to calculate the overall sentiment for each of Trump's tweets. Here's the basic idea:\n",
    "\n",
    "1. For each tweet, find the sentiment of each word.\n",
    "2. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words.\n",
    "\n",
    "First, let's lowercase the text in the tweets since the lexicon is also lowercase. Set the `text` column of the `trump` DF to be the lowercased text of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8bb2469a666ad8f1388abb6ad808881",
     "grade": false,
     "grade_id": "q6b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# set to lower case and replace \\n with space (line breaks)\n",
    "trump['text'] = trump['text'].str.lower()\n",
    "trump['text'] = trump['text'].str.replace('\\n', ' ')\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13de085e42b82bf985f60f6489ed6946",
     "grade": true,
     "grade_id": "q6b-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q05b"
    ]
   },
   "outputs": [],
   "source": [
    "assert trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b985c716dee1121b3764f29e1648fe14",
     "grade": false,
     "grade_id": "q6c-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6c\n",
    "\n",
    "Now, let's get rid of punctuation since it'll cause us to fail to match words. Create a new column called `no_punc` in the `trump` DF to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be any character that isn't a Unicode word character or a whitespace character. You may want to consult the Python documentation on regexes for this problem.\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2901325706be092abb437c239e7d3d83",
     "grade": false,
     "grade_id": "q6c",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Save your regex in punct_re\n",
    "punct_re = '[^A-Za-z0-9_\\\\s]'\n",
    "trump['no_punc'] = trump['text'].str.replace(punct_re, ' ')\n",
    "trump['no_punc']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acba51f5c1ed85ceafe923184be4be33",
     "grade": true,
     "grade_id": "q6c-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q05c"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(punct_re, str)\n",
    "assert re.search(punct_re, 'this') is None\n",
    "assert re.search(punct_re, 'this is ok') is None\n",
    "assert re.search(punct_re, 'this is\\nok') is None\n",
    "assert re.search(punct_re, 'this is not ok.') is not None\n",
    "assert re.search(punct_re, 'this#is#ok') is not None\n",
    "assert re.search(punct_re, 'this^is ok') is not None\n",
    "assert trump['no_punc'].loc[800329364986626048] == 'i watched parts of  nbcsnl saturday night live last night  it is a totally one sided  biased show   nothing funny at all  equal time for us '\n",
    "assert trump['no_punc'].loc[894620077634592769] == 'on  purpleheartday i thank all the brave men and women who have sacrificed in battle for this great nation   usa   https   t co qmfdlslp6p'\n",
    "# If you fail these tests, you accidentally changed the text column\n",
    "assert trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70d1187aee0930362e42d7b497ee5710",
     "grade": false,
     "grade_id": "q6d-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6d:\n",
    "\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. Use the `no_punc` column of `trump` to create a table called `tidy_format`. The index of the table should be the IDs of the tweets, repeated once for every word in the tweet. It has two columns:\n",
    "\n",
    "1. `num`: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. `word`: The individual words of each tweet.\n",
    "\n",
    "The first few rows of our `tidy_format` table look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>num</th>\n",
    "      <th>word</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>0</td>\n",
    "      <td>i</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>1</td>\n",
    "      <td>think</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>2</td>\n",
    "      <td>senator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>3</td>\n",
    "      <td>blumenthal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>4</td>\n",
    "      <td>should</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "**Note that you'll get different results depending on when you pulled in the tweets.** However, you can double check that your tweet with ID `894661651760377856` has the same rows as ours. Our tests don't check whether your table looks exactly like ours.\n",
    "\n",
    "This will require some rather advanced Pandas hacking, but our solution uses a chain of 5 methods on the `trump` DF.\n",
    "\n",
    "* **Hint 1:** Try looking at the `expand` argument to pandas' `str.split`.\n",
    "\n",
    "* **Hint 2:** Try looking at the `stack()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71f93a81cbd4dd1b2ff47a6d65ef758e",
     "grade": false,
     "grade_id": "q6d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "tidy_format = pd.DataFrame(trump['no_punc']\n",
    "                           .str.split(\"\\\\s+\", expand=True)\n",
    "                           .stack()\n",
    "                          )\n",
    "\n",
    "# @Source Piazza Q6 on reseting index then making it ID again--not so sure why this works but it does!\n",
    "tidy_format.reset_index(inplace=True)\n",
    "tidy_format = tidy_format.set_index('id').rename(columns={'level_1' : 'num', 0 : 'word'})\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78b35d7cc8348ceb6ffbd17591dfd4f1",
     "grade": true,
     "grade_id": "q6d-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q05d"
    ]
   },
   "outputs": [],
   "source": [
    "assert tidy_format.loc[894661651760377856].shape == (27, 2)\n",
    "assert ' '.join(list(tidy_format.loc[894661651760377856]['word'])) == 'i think senator blumenthal should take a nice long vacation in vietnam where he lied about his service so he can at least say he was there'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b170181366bfdd62b6d5df9e487479b3",
     "grade": false,
     "grade_id": "q6e-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6e:\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "Add a `polarity` column to the `trump` table.  The `polarity` column should contain the sum of the sentiment polarity of each word in the text of the tweet.\n",
    "\n",
    "**Hint** you will need to merge the `tidy_format` and `sent` tables and group the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''OLD\n",
    "# Merge tidy_format and sent tables based on word, groupby tweet, then sum each word's polarity.\n",
    "\n",
    "summed_polarity = tidy_format.merge(sent, left_on = 'word', right_index=True)['polarity'].groupby('id').sum()\n",
    "\n",
    "# Add polarity column with above series as values\n",
    "trump['polarity'] = summed_polarity\n",
    "trump.fillna(0, inplace=True)\n",
    "# raise NotImplementedError()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ad778cbb8a86c9255acac2d8799fa96",
     "grade": false,
     "grade_id": "q6e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Merge tidy_format and sent tables based on word, groupby tweet, then sum each word's polarity.\n",
    "\n",
    "summed_polarity = tidy_format.merge(sent, how='inner', left_on = 'word', right_index=True)['polarity'].fillna(0).groupby('id').sum()\n",
    "# Add polarity column with above series as values\n",
    "trump['polarity'] = summed_polarity\n",
    "trump.fillna(0, inplace=True)\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff43fac8ac5a95a2eef4bbb52405ad16",
     "grade": true,
     "grade_id": "q6e-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q05e"
    ]
   },
   "outputs": [],
   "source": [
    "assert np.allclose(trump.loc[744701872456536064, 'polarity'], 8.4)\n",
    "assert np.allclose(trump.loc[745304731346702336, 'polarity'], 2.5)\n",
    "assert np.allclose(trump.loc[744519497764184064, 'polarity'], 1.7)\n",
    "assert np.allclose(trump.loc[894661651760377856, 'polarity'], 0.2)\n",
    "assert np.allclose(trump.loc[894620077634592769, 'polarity'], 5.4)\n",
    "# If you fail this test, you dropped tweets with 0 polarity\n",
    "assert np.allclose(trump.loc[744355251365511169, 'polarity'], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d52c721610f2c6a5e4862a1866e5abc",
     "grade": false,
     "grade_id": "a-note-on-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now we have a measure of the sentiment of each of his tweets! Note that this calculation is rather basic; you can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "\n",
    "Now, run the cells below to see the most positive and most negative tweets from Trump in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ea0c421557a05c5a03c51515a5fdc78",
     "grade": false,
     "grade_id": "negative-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Most negative tweets:')\n",
    "for t in trump.sort_values('polarity').head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0d27a82de94a1ce5cf64a2c72ffb7aa",
     "grade": false,
     "grade_id": "postive-tweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for t in trump.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f2ca7db78e7689cdd731e11c9dcffb9",
     "grade": false,
     "grade_id": "q6g",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 6g\n",
    "\n",
    "Plot the distribution of tweet sentiments broken down by whether the text of the tweet contains `nyt` or `fox`.  Then in the box below comment on what we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eb3b83e9188000a1a52427c06ea3c62",
     "grade": true,
     "grade_id": "q6g-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's process NYT mentions first\n",
    "nyt_tweets = tidy_format[(tidy_format['word'].str.contains('nyt'))]\n",
    "\n",
    "# Check what values are in nyt_tweets\n",
    "print('words with nyt in them:')\n",
    "print(nyt_tweets.word.unique())\n",
    "\n",
    "# We notice that 'anything', 'endakennytd', 'ifdanyt', 'anytime' 'jbnytk3fye' and '2yyy6nyta9' \n",
    "# are not related, so remove them:\n",
    "nyt_tweets = tidy_format[(tidy_format['word'] == 'nyt') |\n",
    "                        (tidy_format['word'].str.contains('nytimes')) |\n",
    "                        (tidy_format['word'].str.contains('nytimesdowd')) |\n",
    "                        tidy_format['word'].str.contains('nytdavidbrooks')\n",
    "                        ]\n",
    "\n",
    "\n",
    "# Now let's process Fox-related tweets:\n",
    "fox_tweets = tidy_format[(tidy_format['word'].str.contains('fox'))]\n",
    "\n",
    "# Check what these values actually are:\n",
    "print('words with fox in them:')\n",
    "print(fox_tweets.word.unique())\n",
    "\n",
    "# We see the print line and thus filter out unrelated fox tweets:\n",
    "fox_tweets = fox_tweets[(fox_tweets['word'] == 'foxnews') |\n",
    "                       (fox_tweets['word'] == 'foxandfriends') |\n",
    "                       (fox_tweets['word'] == 'foxbusiness') |\n",
    "                       (fox_tweets['word'] == 'foxandfrlends') |\n",
    "                       (fox_tweets['word'] == 'fox') |\n",
    "                        (fox_tweets['word'] == 'foxfriendsfirst') |\n",
    "                        (fox_tweets['word'] == 'foxnewssunday') |\n",
    "                        (fox_tweets['word'] == 'foxnation') |\n",
    "                        (fox_tweets['word'] == 'foxbizalert') |\n",
    "                        (fox_tweets['word'] == 'johnrobertsfox') |\n",
    "                        (fox_tweets['word'] == 'dumpfoxnews') |\n",
    "                        (fox_tweets['word'] == 'foxnewspolitics')\n",
    "                       ]\n",
    "\n",
    "# Merge each with trump table to get total polarity of tweet\n",
    "nyt_pol = nyt_tweets.merge(trump, right_index=True, left_index=True)['polarity']\n",
    "fox_pol = fox_tweets.merge(trump, right_index=True, left_index=True)['polarity']\n",
    "\n",
    "# Graph this:\n",
    "sns.distplot(nyt_pol, label = 'NYT tweet polarity')\n",
    "sns.distplot(fox_pol, label = 'Fox tweet polarity')\n",
    "plt.legend()\n",
    "plt.title('NYT vs Fox Tweet Polarity')\n",
    "plt.ylabel('fraction')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88e3a81cb1d24df731e4d72d10f08d9f",
     "grade": false,
     "grade_id": "comment-on-faux-news",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Comment on what you observe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07f011cfd9a34c2b3314f2a3aa210187",
     "grade": true,
     "grade_id": "q6g-written",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Based on our results, we see that Trump tends to talk more positively about Fox News. Though there are some negative tweets, we see the distribution (bimodal in shape) has an average polarity somewhere around +2.5, with relatively similar tails in both positive and negative direction. \n",
    "\n",
    "On the other hand, the NYT gets more negative tweets directed at it. This distribution seems to have an average of around -4 and is somewhat Guassian with a slight bump at 0. Its tails are relatively similar, though we can see that there are no tweets with polarity greater than +2.5. On the other hand, there are some really negative tweets less than -10 polarity! \n",
    "\n",
    "Thus, comparing the two, we get the picture that Trump may favor Fox news (or he at least has a more positive sentiment when tweeting about it). But, as is always the case in Data Science, there are still some things that need to be controlled. Take this tweet as an example (ID = 831830548565852160)\n",
    "\n",
    ">\"the fake news media is going crazy with their conspiracy theories and blind hatred. @msnbc &amp; @cnn are unwatchable. @foxandfriends is great!\"\n",
    "\n",
    "Or this one:\n",
    "\n",
    ">\"rt @foxandfriends: president trump vows america will respond to north korean threats with \"fire &amp; fury\" in a warning to the rogue nation\"\n",
    "\n",
    "These are the #2 and #3 lowest rated polarity tweets that mention fox (#1 is about Megyn Kelly... yikes). As we can see, the polarity measure only lets us analyze the *overall* sentiment of the tweet--and not necesarily the president's sentiment towards each news outlet. So, in fact, Trump may have an even more positive perception of Fox than this data may lead us to believe!\n",
    "\n",
    "This makes sense, and thus must be a caveat when considering this data: polarity does **not** control for context! In the end, despite our best efforts, it is really hard to understand how the context plays a roll in overall sentiment. Only a human brain (or really good ML program) can get a good grasp of this collection of tweets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fd71e88bb7adb7f72069aae6032647d",
     "grade": false,
     "grade_id": "q7-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 7: Engagement\n",
    "\n",
    "## Question 7a\n",
    "\n",
    "Which of Trump's tweets had the most retweets? Were there certain words that often led to more retweets?\n",
    "\n",
    "We can find this out by using our `tidy_format` DataFrame. For each word in the `tidy_format` DF, find out the number of retweets that its tweet got. Filter out words that didn't appear in at least 25 tweets, find out the median number of retweets each word got, and save the top 20 most retweeted words into a DataFrame called `top_20`. Your `top_20` table should have this format:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>retweet_count</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>word</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>fake</th>\n",
    "      <td>22963.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>news</th>\n",
    "      <td>20463.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ds100</th>\n",
    "      <td>20432.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>great</th>\n",
    "      <td>20159.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>class</th>\n",
    "      <td>20121.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_frame = trump['retweet_count'].to_frame()\n",
    "rt_frame = rt_frame.merge(tidy_format, left_index=True, right_index=True)\n",
    "word_counts = rt_frame.groupby('word').count()\n",
    "word_counts = word_counts[word_counts['num'] >= 25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = rt_frame.merge(word_counts, left_on='word', right_index=True).groupby('word').median().sort_values(by = 'retweet_count_x', ascending=False)\n",
    "top_20 = top_20['retweet_count_x'].to_frame()\n",
    "top_20 = top_20.rename(columns= {'retweet_count_x' : 'retweet median'})\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reset1 = tidy_format.reset_index()\n",
    "reset2 = trump.reset_index()\n",
    "merged = reset1.merge(reset2, left_index=True, right_index=True)\n",
    "counts = merged.groupby('word').agg('count')\n",
    "at_least25 = counts[counts['retweet_count'] >= 25]\n",
    "at_least25.merge(merged, left_index = True, right_on = 'word')\n",
    "at_least25\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd0b39f47c0462c1c6da4a8c31df384c",
     "grade": false,
     "grade_id": "q7a",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Narrow down trump df to get retweets\n",
    "rt_frame = trump['retweet_count'].to_frame()\n",
    "\n",
    "# Merge with tidy_frame so we can get retweets by tweet (and word)\n",
    "rt_frame = rt_frame.merge(tidy_format, left_index=True, right_index=True)\n",
    "\n",
    "# Groupby word so we can narrow down to words used more than 25 times\n",
    "word_counts = rt_frame.groupby('word').count()\n",
    "word_counts = word_counts[word_counts['num'] >= 25]\n",
    "\n",
    "# Merge rt_frame with word_counts, get median and then sort counts so we can see\n",
    "# Which words get the most retweets\n",
    "top_20 = rt_frame.merge(word_counts, left_on='word', right_index=True).groupby('word').median().sort_values(by = 'retweet_count_x', ascending=False)\n",
    "top_20 = top_20['retweet_count_x'].to_frame()\n",
    "top_20 = top_20.rename(columns= {'retweet_count_x' : 'retweet_count'})\n",
    "top_20 = top_20.head(20)\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdede715af02f26ce48b6334a5c7dea7",
     "grade": true,
     "grade_id": "q7a-test",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "test",
     "q07a"
    ]
   },
   "outputs": [],
   "source": [
    "#### NOTE This Test is kind of iffy (very variable) - needs review before publishing\n",
    "\n",
    "# Although it can't be guaranteed, it's very likely that the top 7 words will still be\n",
    "# in the top 20 words in the next month.\n",
    "assert 'daca'     in top_20.index\n",
    "assert 'nfl'     in top_20.index\n",
    "assert 'anthem' in top_20.index\n",
    "assert 'fbi'    in top_20.index\n",
    "assert 'russia'    in top_20.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4a12be8f88ac931b51b85a1ffa5cfde",
     "grade": false,
     "grade_id": "bar-chart-results",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Here's a bar chart of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a962cdd19dad39e5f8aa5fe2ff096e4",
     "grade": false,
     "grade_id": "top-retweets",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "top_20['retweet_count'].sort_values().plot.barh(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80fdb9c61b546b8a813a3ba015947106",
     "grade": false,
     "grade_id": "q7b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 7b\n",
    "\n",
    "The phrase \"fake news\" is apparently really popular! We can conclude that Trump's tweets containing \"fake\" and/or \"news\" result in the most retweets relative to words his other tweets. Or can we?\n",
    "\n",
    "Consider each of the statements about possible confounding factors below. State whether each statement is true or false and explain. If the statement is true, state whether the confounding factor could have made \"fake\" and/or \"news\" higher on our list than they should be.\n",
    "\n",
    "1. We didn't restrict our word list to nouns, so we have unhelpful words like \"let\" and \"any\" in our result.\n",
    "1. We didn't remove hashtags in our text, so we have duplicate words (eg. #great and great).\n",
    "1. We didn't account for the fact that Trump's follower count has increased over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "577b78199cd4361dd51b92774e09beb0",
     "grade": true,
     "grade_id": "q7b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written"
    ]
   },
   "source": [
    "1. False: words like 'let' and 'any' do matter for VADER analysis. They help the program pick up on context clues and other things such as this (in readme: \"using degree modifiers to alter sentiment intensity (e.g., intensity boosters such as \"very\" and intensity dampeners such as \"kind of\").\" So 'any' could change the tweet polarity of one like this: \"there is not ANY fake news! Sad!\" In fact, only having nouns would hurt our data. For example, adjectives such as 'bad', 'stupid' or 'bigly' help get more accurate VADER readings. Lastly, we took the median of retweet counts for all these words. Thus, common words are not skewing our data as they get filtered out with an average. \n",
    "2. False: we did remove hashtags! In fact, there are hastags like #fakenews that may actually lead to us undercounting the effect of 'fake' or 'news' on retweets (as it appears as 'fakenews' in our anaylsis)\n",
    "3. True: Trump's follower count has steadily grown. Thus, perhaps a slew of tweets in 2014 containing \"STANFORD\" and \"LOW ENERGY\" was retweeted by 4/5ths of all his followers. However, he only had 5m followers then. Now, when he tweets something to his 50 million followers and only 1/5th tweet it, those words would be more 'popular' than the ones tweeted earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba7fba396f9b97171983b6b3b2f774e2",
     "grade": false,
     "grade_id": "q8",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 8\n",
    "\n",
    "Using the `trump` tweets construct an interesting plot describing a property of the data and discuss what you found below.\n",
    "\n",
    "**Ideas:**\n",
    "\n",
    "1. How has the sentiment changed with length of the tweets?\n",
    "1. Does sentiment affect retweet count?\n",
    "1. Are retweets more negative than regular tweets?\n",
    "1. Are there any spikes in the number of retweets and do the correspond to world events? \n",
    "1. *Bonus:* How many Russian twitter bots follow Trump? \n",
    "\n",
    "You can look at other data sources and even tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My extension of this project:\n",
    "\n",
    "First off, Twitter makes it really hard to do much in terms of tracking users who follow(ed) Trump. It's not easy to find who retweeted/favorited his tweets and who followed him (past 100 users). I tried this to no avail. Furthermore, we cannot see snapshots of this in time. This is key because recently Twitter deleted *thousands and thousands* of bot accounts related to the Internet Research Agency! Once deleted, we cannot access any information about these accounts. I tried to find cached versions, which were of no help (that I could find in limited time). It does make sense why Twitter would not want us seeing this, though. \n",
    "\n",
    "But, there is data from Russian-run Twitter accounts, found here:\n",
    "https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731\n",
    "The crazy part is these are only about 3000 some accounts! In total, they produced over 200k tweets over the span of a year (2016)! So, below I analyze them and bring it back to our Trump tweet and polarity analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read bot user list and bot tweet list as found in folder (predownloaded)\n",
    "bot_users = pd.read_csv('bot_users.csv')\n",
    "bot_tweets = pd.read_csv('bot_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at bot tweets over time & related them to election cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat dates\n",
    "reformat_dates = pd.to_datetime(bot_tweets['created_str'])\n",
    "bot_tweets['date'] = reformat_dates\n",
    "\n",
    "# Get counts\n",
    "counts = bot_tweets.set_index('date').groupby(pd.Grouper(freq='D')).count()['user_key']\n",
    "\n",
    "# Plot graph\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.ylabel('total bot tweets per day')\n",
    "plt.xlabel('date')\n",
    "plt.title('Number of Bot Tweets per day')\n",
    "plt.plot(counts.index, counts, color='teal', label='Number of bot tweets', marker='.')\n",
    "xmin = datetime.datetime(2016, 3, 1)\n",
    "xmax = datetime.datetime(2017, 2, 1)\n",
    "plt.xlim(left=xmin, right=xmax)\n",
    "\n",
    "# Key dates that correspond to large tweet numbers\n",
    "election_date = datetime.datetime(2016, 11, 8)\n",
    "democratic_convention = datetime.datetime(2016, 7, 25)\n",
    "republican_convention = datetime.datetime(2016, 7, 18)\n",
    "wikileaks = datetime.datetime(2016, 10, 7)\n",
    "nyc_bomb = datetime.datetime(2016, 9, 17)\n",
    "clinton_faints = datetime.datetime(2016, 9, 11)\n",
    "last_debate = datetime.datetime(2016, 10, 19)\n",
    "inaug = datetime.datetime(2017, 1, 20)\n",
    "\n",
    "\n",
    "# Plot key date lines\n",
    "plt.axvline(x=election_date, color='#e6194b', label = 'Election', alpha=0.5,linewidth=5)\n",
    "plt.axvline(x=democratic_convention, color='#3cb44b', label = 'Democratic convention', alpha=0.5, linewidth=5)\n",
    "plt.axvline(x=republican_convention, color='#f58231', label = 'Republican Convention', alpha=0.5, linewidth=5)\n",
    "plt.axvline(x=wikileaks, color='#ff9408', label = 'Wikileaks Email Release', alpha=0.5, linewidth=5)\n",
    "plt.axvline(x=nyc_bomb, color='#f032e6', label = 'Bomb Explodes in NYC', alpha=0.5, linewidth=5)\n",
    "plt.axvline(x=last_debate, color='#800000', label = 'Last presidential debate', alpha=0.5, linewidth=5)\n",
    "plt.axvline(x=last_debate, color='#e6194b', label = 'Inauguration', alpha=0.5, linewidth=5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few interesting things here:\n",
    "\n",
    "1. The number of tweets put out by the accounts began to increase almost immediately following Trump accepting the RNC nomination. From there on, the average per day increased dramatically. \n",
    "\n",
    "2. The days with the most tweets tended to coincide with key events that occured on the campaign trail.\n",
    "\n",
    "3. Tweets begin to drop off following the election\n",
    "\n",
    "4. The wikileaks email release coinciding with the highest daily count of bot tweets is interesting. Of course, this is expected as it was a big news story. However, it is rumored that Russia supplied wikileaks with these emails. It could have been a coordinated effort, though more analysis needs to be done to make this assertion.\n",
    "\n",
    "There's a good chance lots of these Tweets relate to the election. After all, Mueller did just release a bunch of charges aimed at the company that ran these bots. Good enough for me! But, of course, there are lots of things we'd need to control for in order to have an accurate analysis (e.g. does this show a concerted effort? Or are these spikes/sentiments just a reflection of how the greater twitter universe felt?). But, anyways, it's still cool to see all this data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times did they mention Trump?:\n",
    "bot_tweets['text'] = bot_tweets['text'].fillna('0')\n",
    "bot_tweets['mentions'] = bot_tweets['mentions'].str.strip('[]')\n",
    "trump_mentions = bot_tweets[bot_tweets['mentions'].str.contains('realdonaldtrump')]\n",
    "mentioncount = trump_mentions['mentions'].count()\n",
    "print('Number of times Trump is mentioned:')\n",
    "print(mentioncount)\n",
    "# trump_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times did bots retweet Trump tweets?\n",
    "# Notice our data isn't so hot--tweet ID is not always included w/ retweets\n",
    "# Thus mentions don't suffice and more looking into the data is needed\n",
    "trump_rts = bot_tweets[bot_tweets['text'].str.contains('RT @realDonaldTrump')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets merge trump and trump rts to see what was retweeted\n",
    "trump_rts['retweeted_status_id'] = trump_rts['retweeted_status_id'].fillna(0)\n",
    "bots_rts = trump.merge(trump_rts, left_index=True, right_on = 'retweeted_status_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the polarity of the Tweets they retreated?\n",
    "# I follow steps as above, modified slightly\n",
    "polarity_bot_rts = bots_rts['polarity']\n",
    "sns.distplot(polarity_bot_rts, label = 'Polarity of Bot Rts', color='#658b38')\n",
    "plt.legend()\n",
    "plt.title('Polarity of Trump Bot Retweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots_rts['polarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughts: graph seems to be a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something interesting about their behavior: They\n",
    "# never once interacted directly with other users (e.g. replying)--just retweets or mentions\n",
    "bot_tweets['in_reply_to_status_id'] = bot_tweets['in_reply_to_status_id'].fillna(False)\n",
    "bot_tweets[bot_tweets['in_reply_to_status_id'] == True].count()['in_reply_to_status_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_tweets['new_tweetid'] = range(1,len(bot_tweets)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Vader analysis on bot tweets\n",
    "bot_tweets['text'] = bot_tweets['text'].str.lower()\n",
    "bot_tweets['text'] = bot_tweets['text'].str.replace('\\n', ' ')\n",
    "bot_tweets['no_punc'] = bot_tweets['text'].str.replace(punct_re, ' ')\n",
    "\n",
    "cleaned_format = pd.DataFrame(bot_tweets['no_punc'].str.split(\"\\\\s+\", expand=True).stack())\n",
    "cleaned_format.reset_index(inplace=True)\n",
    "cleaned_format = cleaned_format.rename(columns={'level_1' : 'num', 0 : 'word'})\n",
    "cleaned_format.set_index('level_0', inplace=True)\n",
    "cleaned_format = cleaned_format.rename(index={'tweetid':'tweetid'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with sent\n",
    "tot_pol = cleaned_format.merge(sent, how='inner', left_on = 'word', right_index=True)['polarity'].fillna(0).groupby('level_0').sum()\n",
    "bot_tweets['polarity'] = tot_pol\n",
    "bot_tweets.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print most negative tweets:\n",
    "print(\"I COMMENTED THIS OUT BECAUSE IT HAS ~~VERY~~ OFFENSIVE TERMS\")\n",
    "print(\"UNCOMMENT AND RUN AT OWN PERIL!\")\n",
    "\n",
    "\n",
    "#print('Most negative tweets:')\n",
    "#for t in bot_tweets.sort_values('polarity').head(10)['text']:\n",
    "#    print('\\n  ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "for t in bot_tweets.sort_values('polarity', ascending=False).head(10)['text']:\n",
    "    print('\\n  ', t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_tweets = cleaned_format[(cleaned_format['word'].str.contains('clinton'))]\n",
    "clinton_pol = clinton_tweets.merge(bot_tweets, right_index=True, left_index=True)\n",
    "sns.distplot(clinton_pol['polarity'], label = 'Clinton tweet polarity', color='#da467d')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "islam_tweets = cleaned_format[(cleaned_format['word'].str.contains('islam'))]\n",
    "islam_pol = islam_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(islam_pol, label = 'Islam tweet polarity', color='#0652ff')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()\n",
    "# islam_pol.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigrant_tweets = cleaned_format[(cleaned_format['word'].str.contains('immigrant'))]\n",
    "immigrant_pol = immigrant_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(immigrant_pol, label = 'Immigrant tweet polarity', color='#980002')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()\n",
    "# immigrant_pol.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkeley_tweets = cleaned_format[(cleaned_format['word'].str.contains('berkeley'))]\n",
    "berkeley_pol = berkeley_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(berkeley_pol, label = 'Berkeley tweet polarity', color='#FDB515')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_tweets = cleaned_format[(cleaned_format['word'].str.contains('obama'))]\n",
    "obama_pol = obama_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(obama_pol, label = 'Obama tweet polarity', color='#1fa774')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liberal_tweets = cleaned_format[(cleaned_format['word'].str.contains('liberal'))]\n",
    "liberal_pol = liberal_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(liberal_pol, label = 'liberal tweet polarity', color='#86775f')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_tweets = cleaned_format[(cleaned_format['word'].str.contains('republican'))]\n",
    "republican_pol = republican_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "sns.distplot(republican_pol, label = 'Republican tweet polarity', color='#016795')\n",
    "plt.xlim([-10, 10])\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9742f4a99a0e60845ad7dcc60d774fd7",
     "grade": false,
     "grade_id": "plot8-q",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f84a32506abaa59849e1923f60afb03",
     "grade": true,
     "grade_id": "q8-plot",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# see above for way more\n",
    "# Check out whatever word you'd like!\n",
    "def word_pol_search(word):\n",
    "    word_tweets = cleaned_format[(cleaned_format['word'].str.contains(word))]\n",
    "    word_pol = word_tweets.merge(bot_tweets, right_index=True, left_index=True)['polarity']\n",
    "    sns.distplot(word_pol, label = word + ' polarity')\n",
    "    plt.xlim([-10, 10])\n",
    "    plt.legend()\n",
    "    print('mean polarity:')\n",
    "    print(word_pol.mean())\n",
    "    \n",
    "word_pol_search('blacklivesmatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1fdd71a7455526fe4614bc7eba388f5",
     "grade": false,
     "grade_id": "disc8-q",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36550141bfc678e4ad08057239781458",
     "grade": true,
     "grade_id": "q8-disc",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Overall, we see that bot tweet polarity about controversial topics was as expected. Words such as Clinton, liberal, islam and immigrant tend to skew the graph negatively. However, the effect is not that pronounced, it seems. Over a large number of bots, it does converge slightly towards 0. Additionally, I noticed some of the tweeters take positions that are considered 'far left'. \n",
    "\n",
    "It would be more useful to look at this data more deeply (2 days is not enough). For example, there were lots of tweets that get truncated due to twitter formating. Additionally, dealing with the large number of near-zero polarity tweets needs to be better (i.e. some of them are meaningless and should be thrown out). Also, vader analysis is not perfect (for example, it does not pick up on sarcasm e.g. 'religion of peace', or context). I discuss this in one of the earlier problems. \n",
    "\n",
    "Working with this 200,000 some set of tweets along with the Trump data set is pretty interesting. I am curious to know how many people actually read and interacted with these 3000 bots. Hopefully more data gets released from the investigators as time goes on. \n",
    "\n",
    "(I discuss a few other things in my work for part 8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cd0d0cd6110eba1fea6f1316c48f4a0",
     "grade": false,
     "grade_id": "submission",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "Congrats, you just finished Project 1!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
